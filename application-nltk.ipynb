{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed10611b",
   "metadata": {},
   "source": [
    "# NLTK-Enhanced Financial Chatbot Project\n",
    "\n",
    "---\n",
    "This **Jupyter Notebook** demonstrates the development of a Flask-based chatbot, enhanced with *Natural Language Processing (NLP)* capabilities using NLTK. Our chatbot, **FinBot**, aims to make financial data accessible and actionable through natural language queries.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Setup and Imports](#Setup-and-Imports)\n",
    "- [Initializing Flask and Loading Data](#Initializing-Flask-and-Loading-Data)\n",
    "- [Utinity Functions for NLP Tasks](#Utility-Functions-for-NLP-Tasks)\n",
    "- [Implementing Chatbot Functionality with NLP](#Implementing-Chatbot-Functionality-with-NLP)\n",
    "- [Flask App Routes for User Interaction](#Flask-App-Routes-for-User-Interaction)\n",
    "- [Running the Flask App in a Jupyter Notebook](#Running-the-Flask-App-in-a-Jupyter-Notebook)\n",
    "- [Conclusion](#Conclusion)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3.x\n",
    "- Flask\n",
    "- NLTK\n",
    "- pandas\n",
    "\n",
    "We'll use NLTK for natural language processing to:\n",
    "\n",
    "- Tokenize the query (split it into words and punctuation).\n",
    "- Tag these tokens with part-of-speech tags to help identify nouns, numbers, etc.\n",
    "- Recognize the intent of the query: is the user asking about revenue or earnings?\n",
    "- Extract entities such as company names and years.\n",
    "\n",
    "Before starting, lets ensure that we have downloaded and installed the necessary data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf190bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed required packages.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of packages to install\n",
    "packages = [\"Flask\", \"nltk\", \"pandas\"]\n",
    "\n",
    "# Run pip install\n",
    "result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + packages, capture_output=True)\n",
    "\n",
    "# Check if the installation was successful\n",
    "if result.returncode == 0: # succeeded (returncode is 0) or failed (non-zero returncode)\n",
    "    print(\"Successfully installed required packages.\")\n",
    "else:\n",
    "    print(\"Installation failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150df1dc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cec5ea",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "This section includes all necessary imports and setup instructions for our chatbot. This includes importing libraries and downloading necessary NLTK data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b996840",
   "metadata": {},
   "source": [
    "#### Libraries Overview\n",
    "\n",
    "- **nltk (Natural Language Toolkit)**: A leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "\n",
    "- **Flask**: A lightweight WSGI web application framework. It's designed to make getting started quick and easy, with the ability to scale up to complex applications. For our chatbot, Flask will handle HTTP requests and serve our web interface.\n",
    "\n",
    "- **pandas**: An open-source data analysis and manipulation tool, built on top of the Python programming language. It offers data structures and operations for manipulating numerical tables and time series, crucial for handling our financial dataset.\n",
    "\n",
    "- **threading**: This module constructs higher-level threading interfaces on top of the lower-level thread module. We'll use it to run our Flask application in a background thread within the Jupyter Notebook environment, allowing interactive chatbot testing without interrupting the notebook's execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b02db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from threading import Thread  # For multi-threading support\n",
    "\n",
    "# Flask framework for building web applications\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "# Pandas for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK for natural language processing tasks\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  # Tokenizing text into words\n",
    "from nltk.tag import pos_tag  # Assigning parts of speech to tokens\n",
    "from nltk.corpus import wordnet as wn  # Accessing WordNet lexical database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af0848f",
   "metadata": {},
   "source": [
    "#### Downloading NLTK Data\n",
    "\n",
    "For processing natural language and understanding the structure and meaning of the user's queries, we will download the below datasets:\n",
    "- `punkt`: A tokenizer that divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.\n",
    "- `averaged_perceptron_tagger`: A part-of-speech tagger that uses the averaged perceptron algorithm to tag words with their parts of speech.\n",
    "- `wordnet`: A large lexical database of English. Nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f0862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 11001] getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('averaged_perceptron_tagger')  # For part-of-speech tagging\n",
    "nltk.download('wordnet')  # For synonym lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d57448",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af9b08c",
   "metadata": {},
   "source": [
    "## Initializing Flask and Loading Data\n",
    "Here, we initialize our Flask app and load the dataset that our chatbot will use to find financial information about companies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1165d53",
   "metadata": {},
   "source": [
    "#### Initializing the Flask Application\n",
    "\n",
    "We initialize our Flask application by creating an instance of the `Flask` class. This instance acts as the central registry for our chatbot's routes, views, and other web functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46850a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6168a18",
   "metadata": {},
   "source": [
    "#### Loading the Financial Dataset\n",
    "\n",
    "Our chatbot will use the dataset named `detailed_financial_data.csv`, which contains financial information for various companies across different fiscal years which we created in an earlier notebook. We will use pandas to load this CSV file into a DataFrame, making the data easily accessible for our chatbot's logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a22ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('detailed_financial_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a85f52",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29279f92",
   "metadata": {},
   "source": [
    "## Utility Functions for NLP Tasks\n",
    "\n",
    "Natural Language Processing (NLP) is key to making our chatbot understand the nuances of human language. By implementing utility functions for common NLP tasks, we can enhance our chatbot's ability to parse user queries, identify key information (like company names and financial metrics), and generate relevant responses. In this section, we'll create functions for finding synonym, intent recognition, and entity extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55dc883",
   "metadata": {},
   "source": [
    "#### Synonym Identification\n",
    "\n",
    "To make our chatbot more flexible and understanding, it's helpful to identify synonyms of key words in user queries. This allows the chatbot to recognize various ways users might ask about the same concept. We'll use WordNet, a lexical database for the English language, to find synonyms.\n",
    "\n",
    "**Example Usage:**\n",
    "\n",
    "- **Input Word:** \"profit\"\n",
    "- **Synonyms:** \"earnings\", \"income\", \"returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2671b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    Fetch synonyms for a given word using NLTK's WordNet.\n",
    "\n",
    "    Args:\n",
    "    - word (str): The word for which synonyms are to be found.\n",
    "\n",
    "    Returns:\n",
    "    - set: A set containing synonyms of the given word.\n",
    "    \"\"\"\n",
    "    # Create an empty set called 'synonyms'. This is where we'll store all the unique synonyms we find.\n",
    "    synonyms = set()\n",
    "\n",
    "    # Loop through each synset (a group of synonymous words) that WordNet has for the given 'word'.\n",
    "    for syn in wn.synsets(word):\n",
    "\n",
    "        # Inside each synset, loop through each lemma. A lemma is basically a synonym in the synset.\n",
    "        for lemma in syn.lemmas():\n",
    "\n",
    "            # Add the synonym to our 'synonyms' set. Before adding, replace any underscores ('_') \n",
    "            # in the synonym name with spaces, as WordNet uses underscores to denote spaces.\n",
    "            synonyms.add(lemma.name().replace('_', ' '))\n",
    "\n",
    "    # Return the set of synonyms we've collected.\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52fe5c",
   "metadata": {},
   "source": [
    "#### Intent Recognition\n",
    "\n",
    "To accurately respond to queries, the chatbot needs to discern the user's intent, like whether the query pertains to \"Total Revenue\" or \"Net Income\". The `intent_recognition` function analyzes the query for keywords and their synonyms related to these financial metrics.\n",
    "\n",
    "**Example Flow:**\n",
    "\n",
    "1. **User Query:** \"What was the net income for Apple in 2022?\"\n",
    "2. **Identified Intent:** \"Net Income\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d066aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intent_recognition(query):\n",
    "    \"\"\"\n",
    "    Determine the intent of the query based on keyword synonyms.\n",
    "\n",
    "    Args:\n",
    "    - query (str): User's query as input.\n",
    "\n",
    "    Returns:\n",
    "    - str: Identified financial metric ('Total Revenue' or 'Net Income'), or None if unrecognized.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extend the set of synonyms for 'revenue' with 'sales', which is also commonly used to indicate revenue.\n",
    "    revenue_synonyms = get_synonyms('revenue') | {'sales'}\n",
    "    \n",
    "    # Extend the set of synonyms for 'earnings' with 'profit', 'income', and 'net income', \n",
    "    # which are terms often used interchangeably with earnings.\n",
    "    earnings_synonyms = get_synonyms('earnings') | {'profit', 'income', 'net income'}\n",
    "    \n",
    "    # Tokenize the user's query into individual lowercase words to facilitate matching against synonyms.\n",
    "    tokens = word_tokenize(query.lower())\n",
    "    \n",
    "    # Check if any of the tokens (words) in the query match any of the synonyms for revenue.\n",
    "    # If a match is found, return 'Total Revenue' as the intent.\n",
    "    if any(token in revenue_synonyms for token in tokens):\n",
    "        return 'Total Revenue'\n",
    "    \n",
    "    # If no revenue-related tokens are found, check if any tokens match the synonyms for earnings.\n",
    "    # If a match is found, return 'Net Income' as the intent.\n",
    "    elif any(token in earnings_synonyms for token in tokens):\n",
    "        return 'Net Income'\n",
    "    \n",
    "    # If no matches are found in either synonym set, return None to indicate the intent is unrecognized.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d471ec",
   "metadata": {},
   "source": [
    "#### Extracting Entities from Queries\n",
    "\n",
    "For the chatbot to provide specific financial data, it must identify the company and fiscal year mentioned in a query. The `extract_entities` function parses tokenized and tagged queries to find and return these critical pieces of information.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- **User Query:** \"How much did Tesla earn in 2021?\"\n",
    "- **Extracted Entities:** Company - \"Tesla\", Year - 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acf8582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(tagged_tokens):\n",
    "    \"\"\"\n",
    "    Extract potential company names and fiscal years from tagged tokens.\n",
    "\n",
    "    Args:\n",
    "    - tagged_tokens (list of tuples): Tokenized and part-of-speech tagged tokens.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (company, year) where 'company' is the first proper noun and 'year' is the first four-digit number found, or None if not found.\n",
    "    \"\"\"\n",
    "    # Initialize variables to store the first found company name and fiscal year.\n",
    "    company, year = None, None\n",
    "    \n",
    "    # Loop through each tagged token.\n",
    "    for word, tag in tagged_tokens:\n",
    "        # Check if the current word is tagged as a proper noun ('NNP') and we haven't found a company name yet.\n",
    "        if tag == 'NNP' and not company:\n",
    "            # If conditions are met, this is the first proper noun encountered, so we treat it as the company name.\n",
    "            company = word\n",
    "        \n",
    "        # Check if the current word is a digit, exactly four characters long, and we haven't found a year yet.\n",
    "        elif word.isdigit() and len(word) == 4 and not year:\n",
    "            # If conditions are met, this is the first four-digit number encountered, so we treat it as the fiscal year.\n",
    "            # Convert the string to an integer for the year.\n",
    "            year = int(word)\n",
    "    \n",
    "    # After processing all tokens, return the first found company name and fiscal year (if any).\n",
    "    return company, year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22dfe94",
   "metadata": {},
   "source": [
    "Integrating these advanced NLP utilities enhances our chatbot's workflow, enabling it to:\n",
    "\n",
    "1. **Comprehend a broader range of user queries** through synonym identification.\n",
    "2. **Determine the user's intent** with greater accuracy.\n",
    "3. **Extract key information** needed to fetch relevant financial data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8869f6",
   "metadata": {},
   "source": [
    "## Implementing Chatbot Functionality with NLP\n",
    "\n",
    "Building upon our foundation of NLP utilities, we've developed a core function, `nltk_enhanced_chatbot`, which integrates these tools to process user queries effectively. This function is where our chatbot's NLP capabilities converge to process user queries. When a user submits a query, the chatbot undergoes a series of steps to understand the query and generate an appropriate response. Below is an outline of this process:\n",
    "\n",
    "1. **Query Reception**: The chatbot receives the user's natural language query.\n",
    "2. **Tokenization**: The query is broken down into individual words or tokens.\n",
    "3. **Part-of-Speech Tagging**: Each token is tagged with its respective part of speech, aiding in understanding the query's grammatical structure.\n",
    "4. **Intent Recognition**: The chatbot determines the query's intent, such as whether the user is asking about total revenue, net income, etc.\n",
    "5. **Entity Extraction**: Key information like company names and fiscal years are extracted from the query.\n",
    "6. **Data Retrieval**: Based on the recognized intent and extracted entities, the chatbot fetches the relevant financial data from the dataset.\n",
    "7. **Response Generation**: The chatbot crafts a response incorporating the retrieved data, which is then presented to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f3d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_enhanced_chatbot(user_query):\n",
    "    \"\"\"\n",
    "    Process a user's query to determine the company's total revenue or net income for a given year.\n",
    "\n",
    "    Args:\n",
    "    - user_query (str): The user's text query.\n",
    "\n",
    "    Returns:\n",
    "    - str: Response generated based on the query's intent and extracted entities.\n",
    "    \"\"\"\n",
    "    # Tokenize the user query into individual words.\n",
    "    tokens = word_tokenize(user_query)\n",
    "    \n",
    "    # Tag each token with its part of speech to understand the structure of the sentence.\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Determine the intent of the query (e.g., asking about revenue or net income) using a predefined function.\n",
    "    metric = intent_recognition(user_query)\n",
    "    \n",
    "    # Extract potential company name and fiscal year from the tagged tokens.\n",
    "    company, year = extract_entities(tagged)\n",
    "    \n",
    "    # If a company name, year, and financial metric were successfully identified in the query...\n",
    "    if company and year and metric:\n",
    "        # Example: Query a DataFrame 'df' containing financial data to find the requested information.\n",
    "        # This filters the data for the specified company and year.\n",
    "        data = df[(df['Company'].str.lower() == company.lower()) & (df['Fiscal Year'] == year)]\n",
    "        \n",
    "        # If matching data is found, extract and format the financial metric value to respond to the user.\n",
    "        if not data.empty:\n",
    "            value = data.iloc[0][metric]\n",
    "            return f\"The {metric.lower()} for {company} in {year} was ${value} billion.\"\n",
    "        else:\n",
    "            # If no data matches the query, inform the user.\n",
    "            return \"Data not found for your query. Please check the company name and fiscal year.\"\n",
    "    else:\n",
    "        # If the query could not be understood (missing or unclear company name, year, or metric), ask for clarity.\n",
    "        return \"Sorry, I couldn't understand your query. Please ask about a company's total revenue or earnings for a specific year.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5000b",
   "metadata": {},
   "source": [
    "#### Example Interaction: Query to Response\n",
    "\n",
    "Consider a user query: \"What was Microsoft's revenue in 2021?\"\n",
    "\n",
    "1. **Tokenization**: \"What\", \"was\", \"Microsoft's\", \"revenue\", \"in\", \"2021\"\n",
    "2. **POS Tagging**: [(\"What\", \"WP\"), (\"was\", \"VBD\"), (\"Microsoft's\", \"NNP\"), (\"revenue\", \"NN\"), (\"in\", \"IN\"), (\"2021\", \"CD\")]\n",
    "3. **Intent Recognition**: The chatbot identifies the intent as querying for \"Total Revenue\".\n",
    "4. **Entity Extraction**: Company = \"Microsoft\", Year = 2021\n",
    "5. **Data Retrieval**: Looks up the total revenue for Microsoft in 2021 within the dataset.\n",
    "6. **Response Generation**: \"Microsoft's total revenue in 2021 was $143 billion.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d9734",
   "metadata": {},
   "source": [
    "## Flask App Routes for User Interaction\n",
    "\n",
    "The Flask framework enables our chatbot to communicate with users via a web interface. By defining routes in our Flask app, we can specify how the chatbot should handle incoming HTTP requests and generate responses. This section outlines the key routes that facilitate user interaction with our NLTK-enhanced chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779afaa",
   "metadata": {},
   "source": [
    "#### The Home Route (`/`)\n",
    "\n",
    "The home route is the primary entry point to our chatbot. When users visit this route, they are greeted with the chatbot's interface, which is rendered using an HTML template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd98bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68f403",
   "metadata": {},
   "source": [
    "The `index.html` template contains the HTML and JavaScript necessary for users to interact with the chatbot. It provides a text input field for users to enter their queries and a submit button to send these queries to the chatbot for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b503f14",
   "metadata": {},
   "source": [
    "#### The Chatbot Route (`/chatbot`)\n",
    "\n",
    "The `/chatbot` route handles the core functionality of our chatbot: processing user queries and generating responses. It listens for POST requests, which contain the user's query in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8a835fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/chatbot', methods=['POST'])\n",
    "def chatbot_response():\n",
    "    data = request.get_json()\n",
    "    user_query = data.get('query', '')\n",
    "    response = nltk_enhanced_chatbot(user_query)\n",
    "    return jsonify({'response': response})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f07baf2",
   "metadata": {},
   "source": [
    "Upon receiving a query, this route invokes the `nltk_enhanced_chatbot` function, passing in the user's query. The function processes the query using natural language processing (NLP) techniques, determines the intent, extracts relevant information, and retrieves the appropriate response from the dataset. The response is then returned to the user in JSON format, which the front-end interface displays.\n",
    "\n",
    "\n",
    "#### Interaction Flow:\n",
    "Below is the interaction flow between the user, the Flask app routes, and the chatbot:\n",
    "\n",
    "1. **User submits query**: The user types a query in the web interface and submits it.\n",
    "2. **Home route (`/`)**: Serves the chatbot interface to the user.\n",
    "3. **Chatbot route (`/chatbot`)**: Receives the user's query, processes it, and returns the chatbot's response.\n",
    "4. **Display Response**: The web interface displays the chatbot's response to the user.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa7f48",
   "metadata": {},
   "source": [
    "## Running the Flask App in a Jupyter Notebook\n",
    "\n",
    "Deploying our Flask-based chatbot directly from a Jupyter Notebook presents unique challenges, chiefly how to run the Flask server in a way that doesn't interfere with the notebook's interactivity. By leveraging Python's `threading` module, we can start the Flask app in a separate thread, allowing both the server and the notebook to run concurrently. This setup is particularly useful for development and testing purposes.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "To achieve this, we define a `run_app` function that specifies the Flask app's running configuration. Then, we create and start a thread targeting this function. Here's how it's done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc2808d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5001/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [27/Mar/2024 09:43:04] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Mar/2024 09:43:20] \"POST /chatbot HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "def run_app():\n",
    "    app.run(port=5001, debug=False, use_reloader=False)\n",
    "\n",
    "flask_thread = Thread(target=run_app)\n",
    "flask_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f4d7d9",
   "metadata": {},
   "source": [
    "By setting `debug=False` and `use_reloader=False`, we ensure that the Flask app does not attempt to restart within the notebook environment, which could cause issues. The chosen port, `5001`, can be adjusted based on your preferences and any potential port conflicts on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b6eb8",
   "metadata": {},
   "source": [
    "#### Best Practices and Considerations\n",
    "\n",
    "- **Port Selection**: Ensure the chosen port is free and not blocked by other applications.\n",
    "- **Debugging**: Running Flask in debug mode within a Jupyter Notebook is not recommended due to potential conflicts with the notebook server. Keep `debug=False` for stability.\n",
    "- **Stopping the Server**: To stop the Flask server, you may need to manually interrupt the kernel or close the Jupyter Notebook. Be aware that the thread will continue to run until explicitly stopped or until the notebook session ends.\n",
    "This setup is good enough for development  but for production deployment, Flask apps should be run in a more robust server environment, such as with Gunicorn or uWSGI behind Nginx or Apache.\n",
    "\n",
    "#### Testing the Chatbot\n",
    "\n",
    "To interact with the chatbot:\n",
    "1. Start the Flask app by running it in the terminal or command prompt.\n",
    "2. Navigate to the home route on the web browser to access the chat interface.\n",
    "3. Enter a query and submit it to see the chatbot's response.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0351ad2e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We developed an NLTK-enhanced financial chatbot, leveraging the robust capabilities of Flask, pandas, and the Natural Language Toolkit (NLTK). \n",
    "\n",
    "**Key Achievements:**\n",
    "\n",
    "- **Integration of NLP Techniques:** We successfully integrated advanced NLP techniques to enhance our chatbot's understanding of user queries, allowing it to process and respond to inquiries about financial data with a high degree of accuracy.\n",
    "- **Flask for Web Interactivity:** By utilizing Flask, we demonstrated how to effectively build and deploy a web-based chatbot, making sophisticated data analysis accessible through simple natural language queries.\n",
    "- **Interactive Testing in Jupyter:** The innovative use of threading to run the Flask application within a Jupyter Notebook environment underscored the notebook's utility as a powerful tool for interactive development and testing.\n",
    "\n",
    "**Learnings and Reflections:**\n",
    "\n",
    "This project underscored the importance of clean, structured data and the power of NLP in extracting meaningful information from user input. It also showcased the challenges and solutions in deploying interactive applications directly from a Jupyter Notebook, providing valuable insights into the development workflow and debugging practices.\n",
    "\n",
    "**Looking Ahead:**\n",
    "\n",
    "- **Enhancing NLP Capabilities:** Future work could explore more sophisticated NLP models and techniques, such as named entity recognition (NER) and sentiment analysis, to further improve the chatbot’s understanding and responses.\n",
    "- **Expanding the Dataset:** Incorporating a more extensive and diverse financial dataset would enable the chatbot to provide insights across a broader spectrum of companies and financial metrics.\n",
    "- **Deployment and Scaling:** Moving beyond the notebook, deploying the chatbot on a cloud platform would be a crucial step towards scaling and making the application widely accessible.\n",
    "\n",
    "**Closing Thoughts:**\n",
    "\n",
    "This development showcases the incredible potential of combining NLP with web technologies to create intelligent, user-friendly applications. As I continue to refine and expand this chatbot, it serves as a testament to the power of collaboration between data science and web development disciplines, driving forward the possibilities of technology and innovation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97608a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
